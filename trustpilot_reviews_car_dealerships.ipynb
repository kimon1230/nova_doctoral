{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrustPilot.com User Reviews\n",
    "\n",
    "This notebook processes the reviews for all businesses in the USA that are listed in the `Car Dealership` category and have received a review. Each step in the process tries to clean up the data before storing in a .json file at the end\n",
    "\n",
    "These are the steps:\n",
    "1. Create a list of all businesses in the category with at least 1 review\n",
    "    - Filter to US businesses\n",
    "    - Only verified & claimed business\n",
    "    - Results sorted by number of reviews in descending order\n",
    "2. For each business, parse the HTML via *BeautifulSoup4* and capture the reviews\n",
    "    - Paginate through each result set as needed\n",
    "    - Ignore blank or non-english comments\n",
    "4. Store data in a .json file\n",
    "\n",
    "This notebook is using BeautifulSoup4 for all the HTML parsing since the web server generates the final code without any dynamic JavaScript altering the results. This makes scraping much easier and straightforward. Were this not the case, a solution using [Selenium](https://github.com/SeleniumHQ/Selenium) would be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non default libraries used\n",
    "In case your environment does not have these libraries, execute the following:\n",
    "`pip install beautifulsoup4 requests langdetect`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries used in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "from langdetect import detect\n",
    "from langdetect import LangDetectException\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = \"https://www.trustpilot.com/categories/car_dealer?claimed=true&sort=reviews_count&verified=true\" #filtered & sorted\n",
    "base_url = \"https://www.trustpilot.com\"\n",
    "all_reviews = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape the entire category\n",
    "With this function, we start at the provided URL (in this case the car dealership page) and find all those businesses that are listed and have any reviews posted. Since the web results are paginated, we are using logic to navigate from one page to the next and process each page in succession. The results are being sorted on the server side (via URL parameters) by review count in descending order, therefore once we hit the first business with 0 reviews we know we can stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_category_page(start_url: str):\n",
    "    current_page_url = start_url\n",
    "    business_urls = []\n",
    "\n",
    "    while current_page_url:\n",
    "        response = requests.get(current_page_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        for link in soup.find_all('a', attrs={'data-business-unit-card-link': True}):\n",
    "            tst = link.find_all('img', alt=lambda value: value and \"TrustScore\" in value)\n",
    "            if len(tst) == 0: return business_urls #since we're sorting by number of reviews in descending order, when we hit 0 we're done\n",
    "            url = link.get('href')\n",
    "            url = urljoin(base_url, url)\n",
    "            if '/review/' in url:\n",
    "                business_urls.append(url)\n",
    "\n",
    "        next_page_link = soup.find('a', attrs={'data-pagination-button-next-link': 'true'})\n",
    "        if next_page_link and next_page_link.get('href'):\n",
    "            next_page_url = next_page_link.get('href')\n",
    "            next_page_url = urljoin(base_url, next_page_url)\n",
    "            current_page_url = next_page_url\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "    \n",
    "    return business_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape each business' page\n",
    "Here we navigate to the specific business' page and start processing the reviews one page at a time. In an effort to be thoughtful toward the webserver, we have a random 2-7 second wait between page request. Note that there are businesses with thousands of pages.\n",
    "\n",
    "As we loop through each review, we do work to remove empty reviews and any that are not in English. We then return the resulting `Dict` for processing by the main body of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract business name and reviews from a given URL\n",
    "def get_business_reviews(url: str):\n",
    "    reviews_data = []\n",
    "\n",
    "    while url: #loop while we have a valid URL to process\n",
    "        try:\n",
    "            #get the page and load into BeautifulSoup\n",
    "            response = requests.get(url) \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # This assumes 'business_name' remains constant across pagination\n",
    "            if not reviews_data:  # Only get the business name on the first page\n",
    "                busElement = soup.find('div', id='business-unit-title')\n",
    "                if busElement:\n",
    "                    business_name = busElement.text.strip()\n",
    "        \n",
    "            #iterate through all the reviews for the business\n",
    "            for review in soup.find_all('article', attrs={'data-service-review-card-paper': 'true'}):\n",
    "                try:\n",
    "                    review_rating = review.find('div', attrs={'data-service-review-rating':True})\n",
    "                    review_title = review.find('h2', attrs={'data-service-review-title-typography': 'true'})\n",
    "                    review_text = review.find('p', attrs={'data-service-review-text-typography': 'true'})\n",
    "\n",
    "                    if review_rating and (review_text or review_title): #we only want the data if we have both a rating and either a review or a title\n",
    "                        rating = review_rating['data-service-review-rating']\n",
    "                        ttl = review_title.text.strip() if review_title else None\n",
    "                        txt = review_text.text.strip() if review_text else None\n",
    "                        if (ttl and detect(ttl)=='en') or (txt and detect(txt)=='en'): #we only want the data if it's in English\n",
    "                            reviews_data.append({'rating': rating, 'review_title': ttl, 'review_text': txt})\n",
    "                except LangDetectException:\n",
    "                    pass #ignore this particular type of error\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "            \n",
    "            # Find the 'Next' page link and update `url` for the next iteration\n",
    "            next_page_link = soup.find('a', attrs={'data-pagination-button-next-link': 'true'})\n",
    "            if next_page_link and 'href' in next_page_link.attrs:\n",
    "                url = urljoin(base_url, next_page_link['href']) \n",
    "            else:\n",
    "                url = None\n",
    "        \n",
    "            time.sleep(random.uniform(2, 7)) #be nice and wait 2-7 seconds\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e}\")\n",
    "            break  # Exit loop on request error\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            break  # Exit loop on any other error\n",
    "        \n",
    "    return {'business_name': business_name, 'reviews': reviews_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start of the main body\n",
    "We start by pulling the list of all businesses with reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_urls = scrape_category_page(start_url)\n",
    "print(f\"Found {len(business_urls)} business review URLs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now iterate through each resulting URL but do it sorted by review count in ascending order. This is to ensure that if we encounter any issues in pulling the data, we discover it with small datasets before committing to the multi-thousand page businesses.\n",
    "\n",
    "We try to be nice and wait 30-60 seconds before starting the process of the next business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in reversed(business_urls): #since the webserver returns the results sorted by review count in descending order, we reverse the list\n",
    "    ts = datetime.datetime.now()\n",
    "    print(f\"{ts}\\tNow processing: {url}\")\n",
    "\n",
    "    reviews = get_business_reviews(url) #get the reviews for the business\n",
    "\n",
    "    try:\n",
    "        if len(reviews['reviews']) > 0: #only store the data if we got anything\n",
    "            all_reviews.append(reviews)\n",
    "            ts = datetime.datetime.now()\n",
    "            print(f\"{ts}\\tProcessed {len(reviews['reviews'])} reviews\")\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"An error occurred parsing {url}: {e}\")\n",
    "\n",
    "    time.sleep(random.uniform(30, 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're done pulling all the data from the web server, we need to do some further cleanup by removing invalid characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_total = 0\n",
    "delim=\"Reviews\"\n",
    "for rvw in all_reviews:\n",
    "    txt = rvw['business_name']\n",
    "    bsnm = txt.split(delim)[0]\n",
    "    rvw['business_name'] = bsnm.replace('\\u00A0', '') #for some reason this character is included in the text and we need to remove it\n",
    "    print(f\"Business Name: {rvw['business_name']}\\tReviews: {len(rvw['reviews'])}\")\n",
    "    running_total += len(rvw['reviews'])\n",
    "\n",
    "print(f\"Total reviews: {running_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the collected data to a JSON file with UTF-8 encoding\n",
    "filename = f\".\\\\trusted_pilot_car_dealerships-{datetime.datetime.now().strftime('%Y%m%d')}.json\"\n",
    "with open(filename, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "    json.dump(all_reviews, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
